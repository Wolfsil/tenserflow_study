# -*- coding: utf-8 -*-
"""final_test

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uQnf56bem0wvQIdx-8NHs36G1n7h_1O2
"""

import tensorflow as tf
from tensorflow import keras 
import matplotlib.pyplot as plt
import glob
import os
from pathlib import Path
import numpy as np 


from google.colab import drive
drive.mount('/content/drive')

#-----------------------------------------
#간단하게 이미지 전처리 부터 시작해보자
dataset_dir = '/content/drive/MyDrive/Colab Notebooks/Oxford-IIIT_Pet_Dataset'
image_paths = glob.glob('{}/images/*.jpg'.format(dataset_dir))
image_paths.sort()
print(image_paths)

"""수집한 경로명들을 `trainval.txt` 파일과 `test.txt` 파일의 내용에 따라 분류한다."""

trainval_list = open('{}/annotations/trainval.txt'.format(dataset_dir), 'r')
Lines = trainval_list.readlines()
train_data_file_name = [line.split()[0] for line in Lines]

test_list = open('{}/annotations/test.txt'.format(dataset_dir), 'r')
Lines2 = test_list.readlines()
test_data_file_name = [line.split()[0] for line in Lines2]

trainval_image_paths = [p for p in image_paths if Path(p).stem in train_data_file_name]
test_image_paths = [p for p in image_paths if Path(p).stem in test_data_file_name]
print(len(test_image_paths))

#------------------------
#(추가)오류 이미지 탐색 코드
#문제와 관계 없으므로 주석
# def test_if_valid_jpeg(path):
#   img = tf.io.read_file(path)
#   image = bytearray(img.numpy())
#   if image[0] == 255 and image[1] == 216 and image[-2] == 255 and image[-1] == 217:
#     return True
#   else:
#     return False

# for img_path  in trainval_image_paths:
#   if not test_if_valid_jpeg(img_path):
#     print(img_path)
# for img_path in test_image_paths:
#   if not test_if_valid_jpeg(img_path):
#     print(img_path)

#오류 이미지 제거 코드(사실 코드로 구현하기 보단, 손으로 직접하는게 편하다. 6개 정도니까. 다만, 프로그래머가 그러는건 좀........)
#문제와 관계없으므로 주석
# def remove_trash(path_list):
#   for path in path_list:
#     # shutil.rmtree(path)
#     os.remove(path)

# a=[r'/content/drive/MyDrive/Colab Notebooks/Oxford-IIIT_Pet_Dataset/images/Egyptian_Mau_156.jpg',r'/content/drive/MyDrive/Colab Notebooks/Oxford-IIIT_Pet_Dataset/images/Egyptian_Mau_186.jpg',r'/content/drive/MyDrive/Colab Notebooks/Oxford-IIIT_Pet_Dataset/images/Abyssinian_5.jpg',r'/content/drive/MyDrive/Colab Notebooks/Oxford-IIIT_Pet_Dataset/images/Egyptian_Mau_138.jpg',r'/content/drive/MyDrive/Colab Notebooks/Oxford-IIIT_Pet_Dataset/images/Egyptian_Mau_14.jpg']
# remove_trash(a)
#-------------------------------------

##강아지=0, 고양이=1
def cat_or_dog(path):
  data_image=[]
  labels = [] 
  countcat=0
  countdog=0
  for line in path:
    data_image.append(line)
    path_split=line.split('/')
    if path_split[-1].lower() == path_split[-1]:
      labels.append(0)
      countdog+=1
    else:
      labels.append(1)
      countcat+=1
  # print("{}강아지. {}고양이.\n".format(countdog,countcat))
  return data_image, labels

trainval_image_paths,trainval_label_paths =cat_or_dog(trainval_image_paths)
test_image_paths,test_label_paths = cat_or_dog(test_image_paths)


train_image_path_ds = tf.data.Dataset.from_tensor_slices(trainval_image_paths)
train_label_path_ds = tf.data.Dataset.from_tensor_slices(trainval_label_paths)
test_image_path_ds = tf.data.Dataset.from_tensor_slices(test_image_paths)
test_label_path_ds = tf.data.Dataset.from_tensor_slices(test_label_paths)

train_dataset = tf.data.Dataset.zip((train_image_path_ds, train_label_path_ds))
test_dataset = tf.data.Dataset.zip((test_image_path_ds, test_label_path_ds))

#역시 강아지가 최고입니다. 이렇게 사진에 잘 찍혀주잖습니까? 수치가 증명하고 있습니다.

EPOCHS = 50
IMG_SIZE=128
CHANNER=3
BATCH_SIZE=64
TEST_LENGTH = len(test_dataset)
VALIDATION_STEPS = TEST_LENGTH//BATCH_SIZE//100
#임의의 스탭으로 해도 괜찮을거란 판단. 연산속도를 위해 100으로 나눕니다.

def normalize(input_image):
  input_image = tf.cast(input_image, tf.float32) / 255.0
  return input_image
  
def load_image_train(img_path,img_label):
  image = tf.io.read_file(img_path)
  image = tf.image.decode_jpeg(image)
  image = tf.image.resize(image, [IMG_SIZE,IMG_SIZE])

  if tf.random.uniform(()) > 0.5:
    image = tf.image.flip_left_right(image)

  image = normalize(image)
  return image,img_label

def load_image_test(img_path, img_label):
  image = tf.io.read_file(img_path)
  image = tf.image.decode_jpeg(image)
  image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])

  image= normalize(image)
  return image,img_label

train_data = train_dataset.map(load_image_train)
test_data = test_dataset.map(load_image_test)

train_ds = train_data.shuffle(5000).batch(64)
test_ds = test_data.shuffle(5000).batch(64)

def create_model():
  model = keras.Sequential([
    keras.layers.Conv2D(32, kernel_size=3, activation='relu',padding='same', 
                        input_shape=(IMG_SIZE, IMG_SIZE, CHANNER)),
    keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2), padding='same'),
    keras.layers.Conv2D(64, kernel_size=3, activation='relu', padding='same'),
    keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same'),
    keras.layers.Conv2D(128, kernel_size=3, activation='relu', padding='same'),
    keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same'),
    keras.layers.Flatten(),
    keras.layers.Dense(256, activation=tf.nn.relu),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(2, activation=tf.nn.softmax)
  ])
  model.compile(optimizer='adam',
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy'])
  # model.summary()
  return model


def train_from_scratch(train_set,test_set):

 model = create_model()
 model.fit(train_set, epochs=EPOCHS ,validation_steps=VALIDATION_STEPS,
                          validation_data=test_set)
 return model



model=train_from_scratch(train_ds,test_ds)

predictions = model.predict(test_ds)

#강아지냐 고양이냐를 구하는 것 까지가 이 문제!
for i in predictions:
  if(i[0]>i[1]):
    print('dog')
  else:
    print('cat')
#결과가 강아지한테 치우쳐진 이유를 발견했다.
#그저 테스트 데이터에는 강아지 데이터 베이스가 많은게 문제였다.
#그러니까.....대충 찍어도 개라고 말하면 얼추 들어맞는다는,,,,,,,,,

#그러므로 평균정확도는
test_loss, test_acc = model.evaluate(test_ds, verbose=2)
print(test_acc)

print("테스트에 대한 정확도는 {}% 입니다.".format(test_acc*100))