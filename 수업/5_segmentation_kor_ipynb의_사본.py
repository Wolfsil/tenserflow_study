# -*- coding: utf-8 -*-
"""5.Segmentation_Kor.ipynb의 사본

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I_u3pMGUSE9G0p50PH-u7ycbdFX2QyiW

# 이미지 세그멘테이션(Image segmentation)

지금까지 이 수업에서는 간단한 regression 문제와 classification 문제를 다루었다. 이번 시간에는 이미지 세그멘테이션 문제를 다룬다. 이 강좌는 텐서플로 홈페이지의 [튜토리얼](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/segmentation.ipynb#scrollTo=sMP7mglMuGT2)을 이 수업의 목적에 맞게 수정한 것이다.

##이미지 세그멘테이션이란?

이미지 세그멘테이션은 이미지를 이미지 내의 오브젝트들로 분할하는 것이다. 즉, 모든 픽셀들을 각 픽셀이 속한 오브젝트에 따라 분류하는 문제이다. 대표적인 예로는 이미지를 주 피사체와 배경으로 분할하는 것을 들 수 있다. 

유사한 색상의 픽셀들을 분할하는 것이 아니라 어떤 동일한 오브젝트를 구성하는 픽셀들을 분할하는 것이므로 시맨틱 세그멘테이션(semantic segmentation)라고 부르기도 한다.

픽섹들을 몇 가지 클래스로 분류할 것인지 미리 지정되어야 한다. 따라서 특성 상황 혹은 분야의 이미지를 대상으로 하는 것이 현실적이다. 예를 들면 자율주행차량의 경우 도로 이미지에서 '도로', '다른 차량', '신호등' 등으로 분류하거나 의료/생물학적 이미지에서 특정 생체조직과 주변 조직을 분할하는 등의 예가 성공적인 사례라고 할 수 있다.

이 튜토리얼에서는 간단한 예로 동물 사진에서 '동물'과 '배경'을 분할하는 경우를 다룬다. 이미지 분할을 위한 잘 알려진 신경망 구조인 [U-net](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/)을 부분적으로 수정하여 사용한다.
"""

import tensorflow as tf
import tensorflow_datasets as tfds
import matplotlib.pyplot as plt
import glob
import os
from pathlib import Path

"""## 데이터셋

이 강좌에서는 [Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/)을 사용한다. 이 데이터셋은 7,000여장의 애완동물 이미지와 각 이미지에 대한 픽셀단위의 마스크로 구성된다. 여기서 마스크는 이미지의 각 픽셀을 3개의 클래스(**동물에 속한 픽셀, 배경 픽셀**, 그리고 **동물과 배경의 경계에 위치한 픽셀**)로 분류한다. 

링크된 페이지를 방문하면 `Dataset`과 `Groundtruth data`를 각각 다운로드 할 수 있다. 각각은 하나의 압축파일이며 두 파일의 압축을 풀어서 적절한 위치에 저장한다. 이하에서는 Google Drive의 적절한 위치에 `Oxford-IIIT_Pet_Dataset` 디렉토리를 만들고 그 안에 `annotations` 디렉토리와 `images` 디렉토리를 저장하였다.

`images` 디렉토리에는 7,000장 정도의 애완동물 이미지가 저장되어 있다. `annotations` 디렉토리의 `trimaps` 디렉토리에 마스트 이미지들이 저장되어 있다. train 데이터와 test 데이터가 따로 분류되어 있지는 않으며, 다만 `annotations` 디렉토리에 `trainval.txt`와 `test.txt`에는 각각 트레인 용으로 사용할 이미지와 테스트용으로 사용할 이미지 파일의 이름이 분류되어 저장되어 있다. 이 분류를 반드시 따를 이유는 없지만 이 강좌에서는 이 분류를 따른다.

Google Drive에 저장된 데이터를 사용하기 위해서 먼저 다음과 같이 마운트한다.
"""

from google.colab import drive
drive.mount('/content/drive')

"""glob 유틸리티를 이용하여 `images` 디렉토리와 `annotations` 디렉토리에 저장된 파일들의 경로명을 수집하고 알파벳 순으로 정렬한다."""

#-----------------------------------------
#간단하게 이미지 전처리 부터 시작해보자
dataset_dir = '/content/drive/MyDrive/Colab Notebooks/Oxford-IIIT_Pet_Dataset'
image_paths = glob.glob('{}/images/*.jpg'.format(dataset_dir))
label_paths = glob.glob('{}/annotations/trimaps/*.png'.format(dataset_dir))
image_paths.sort()
label_paths.sort()
print(image_paths)

"""수집한 경로명들을 `trainval.txt` 파일과 `test.txt` 파일의 내용에 따라 분류한다."""

trainval_list = open('{}/annotations/trainval.txt'.format(dataset_dir), 'r')
Lines = trainval_list.readlines()
train_data_file_name = [line.split()[0] for line in Lines]

test_list = open('{}/annotations/test.txt'.format(dataset_dir), 'r')
Lines2 = test_list.readlines()
test_data_file_name = [line.split()[0] for line in Lines2]

# print(train_data_file_name)

trainval_image_paths = [p for p in image_paths if Path(p).stem in train_data_file_name]
trainval_label_paths = [p for p in label_paths if Path(p).stem in train_data_file_name]

test_image_paths = [p for p in image_paths if Path(p).stem in test_data_file_name]
test_label_paths = [p for p in label_paths if Path(p).stem in test_data_file_name]

#그냥 보여줄 뿐
# from IPython.display import Image 
# for i, p in enumerate(trainval_image_paths):
#   img = Image(filename=p)
#   display(img)
#   if i > 0:
#     break

# for i, p in enumerate(trainval_label_paths):
#   img = Image(filename=p)
#   display(img)
#   if i > 0:
#     break

"""경로명으로 이루어진 Dataset을 생성한다."""

train_image_path_ds = tf.data.Dataset.from_tensor_slices(trainval_image_paths)
train_label_path_ds = tf.data.Dataset.from_tensor_slices(trainval_label_paths)

test_image_path_ds = tf.data.Dataset.from_tensor_slices(test_image_paths)
test_label_path_ds = tf.data.Dataset.from_tensor_slices(test_label_paths)

train_dataset = tf.data.Dataset.zip((train_image_path_ds, train_label_path_ds))
test_dataset = tf.data.Dataset.zip((test_image_path_ds, test_label_path_ds))
print(train_dataset)
#전처리 끝
#------------------------------------------------------------------

#------------------------
#(추가)오류 이미지 탐색 코드
#문제와 관계 없으므로 주석
# def test_if_valid_jpeg(path):
#   img = tf.io.read_file(path)
#   image = bytearray(img.numpy())
#   if image[0] == 255 and image[1] == 216 and image[-2] == 255 and image[-1] == 217:
#     return True
#   else:
#     return False

# for img_path, label_path in train_dataset:
#   if not test_if_valid_jpeg(img_path):
#     print(img_path)
# for img_path, label_path in test_dataset:
#   if not test_if_valid_jpeg(img_path):
#     print(img_path)

#오류 이미지 제거 코드(사실 코드로 구현하기 보단, 손으로 직접하는게 편하다. 6개 정도니까. 다만, 프로그래머가 그러는건 좀........)
#문제와 관계없으므로 주석
# def remove_trash(path_list):
#   for path in path_list:
#     # shutil.rmtree(path)
#     os.remove(path)
# a=[r'/content/drive/MyDrive/Colab Notebooks/Oxford-IIIT_Pet_Dataset/images/Egyptian_Mau_156.jpg',r'/content/drive/MyDrive/Colab Notebooks/Oxford-IIIT_Pet_Dataset/images/Egyptian_Mau_186.jpg',r'/content/drive/MyDrive/Colab Notebooks/Oxford-IIIT_Pet_Dataset/images/Abyssinian_5.jpg',r'/content/drive/MyDrive/Colab Notebooks/Oxford-IIIT_Pet_Dataset/images/Egyptian_Mau_138.jpg',r'/content/drive/MyDrive/Colab Notebooks/Oxford-IIIT_Pet_Dataset/images/Egyptian_Mau_14.jpg']
# remove_trash(a)
#-------------------------------------

#--------------------------------
#이제 이미지를 전처리 한다! gogo

def normalize(input_image, input_mask):
  input_image = tf.cast(input_image, tf.float32) / 255.0
  input_mask -= 1
  return input_image, input_mask

"""트레인 데이터셋의 전처리 함수이다. 이미지 파일을 읽고, 128*128 크기로 리사이즈한다. 1/2의 확률로 이미지를 좌우 반전하여 data augmentation을 수행한다."""

def load_image_train(img_path, mask_path):
    image = tf.io.read_file(img_path)
    image = tf.image.decode_jpeg(image)
    image = tf.image.resize(image, [128, 128])

    mask = tf.io.read_file(mask_path)
    mask = tf.io.decode_png(mask, channels=0, dtype=tf.dtypes.uint8)
    mask = tf.image.resize(mask, [128, 128])

    if tf.random.uniform(()) > 0.5:
      image = tf.image.flip_left_right(image)
      mask = tf.image.flip_left_right(mask)

    image, mask = normalize(image, mask)
    return image, mask

"""테스트데이터에 대해서는 data augmentation을 제외하고 동일하게 한다."""

def load_image_test(img_path, mask_path):
  image = tf.io.read_file(img_path)
  image = tf.image.decode_jpeg(image)
  image = tf.image.resize(image, [128, 128])

  mask = tf.io.read_file(mask_path)
  mask = tf.io.decode_png(mask, channels=0, dtype=tf.dtypes.uint8)
  mask = tf.image.resize(mask, [128, 128])

  image, mask = normalize(image, mask)
  return image, mask

train_ds = train_dataset.map(load_image_train)
test_ds = test_dataset.map(load_image_test)

"""전처리 함수를 테스트해본다."""

def display_images(display_list):
  plt.figure(figsize=(15, 15))

  title = ['Input Image', 'True Mask', 'Predicted Mask']

  for i in range(len(display_list)):
    plt.subplot(1, len(display_list), i+1)
    plt.title(title[i])
    plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))
    plt.axis('off')
  plt.show()

for image, mask in train_ds.take(1):
  sample_image, sample_mask = image, mask

display_images([sample_image, sample_mask])

TRAIN_LENGTH = len(trainval_image_paths)
BATCH_SIZE = 64
BUFFER_SIZE = 1000
STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE

train_dataset = train_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
# train_dataset = train_ds.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()
# train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
test_dataset = test_ds.batch(BATCH_SIZE)
#이미지 전처리 완료!
#-----------------------------------------------------

"""## U-net의 구성

데이터셋이 완성되었다. 이제 [U-net](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/)을 구성해보자. U-net의 구조는 다음과 같다.

<img src="https://github.com/ohheum/DS2020/blob/master/u-net-architecture.png?raw=true" width="600" />
"""

OUTPUT_CHANNELS = 3

def downsample(filters, size, apply_batchnorm=True):
  initializer = tf.random_normal_initializer(0., 0.02)

  result = tf.keras.Sequential()
  result.add(
    tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',
                           kernel_initializer=initializer, use_bias=False))

  if apply_batchnorm:
    result.add(tf.keras.layers.BatchNormalization())

  result.add(tf.keras.layers.LeakyReLU())

  return result

down_stack = [
  downsample(64, 3, apply_batchnorm=False),   # output: (64, 64, 64)
  downsample(128, 3),                         # (32, 32, 128)
  downsample(256, 3),                         # (16, 16, 256)
  downsample(512, 3),                         # (8, 8, 512)
  downsample(512, 3),                         # (4, 4, 512)
]

# 원본 튜토리얼에서는 MobileNetV2에서 몇 개의 pretraining된 층들을 선택하여 U-net의 downstack으로
# 사용하였다. 이렇게 하면 training 시간을 단축할 수 있다.

# base_model = tf.keras.applications.MobileNetV2(input_shape=[128, 128, 3], include_top=False)
#
# # Use the activations of these layers
# layer_names = [
#     'block_1_expand_relu',   # 64x64
#     'block_3_expand_relu',   # 32x32
#     'block_6_expand_relu',   # 16x16
#     'block_13_expand_relu',  # 8x8
#     'block_16_project',      # 4x4
# ]
# layers = [base_model.get_layer(name).output for name in layer_names]
#
# # Create the feature extraction model
# down_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)
# down_stack.trainable = False



def upsample(filters, size, apply_dropout=False):
  initializer = tf.random_normal_initializer(0., 0.02)

  result = tf.keras.Sequential()
  result.add(
    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,
                                    padding='same',
                                    kernel_initializer=initializer, use_bias=False))

  result.add(tf.keras.layers.BatchNormalization())

  if apply_dropout:
    result.add(tf.keras.layers.Dropout(0.5))

  result.add(tf.keras.layers.ReLU())
  return result

up_stack = [
  upsample(512, 3),   # 4x4x512 -> 8x8x512
  upsample(256, 3),   # 8x8x -> 16x16x256
  upsample(128, 3),   # 16x16x -> 32x32x128
  upsample(64, 3),    # 32x32x -> 64x64x64
]

def unet_model(output_channels):
  inputs = tf.keras.layers.Input(shape=[128, 128, 3])
  x = inputs

  # Downsampling through the model
  skips = []
  for down in down_stack:
    x = down(x)
    skips.append(x)

  skips = reversed(skips[:-1])

  # Upsampling and establishing the skip connections
  for up, skip in zip(up_stack, skips):
    x = up(x)
    concat = tf.keras.layers.Concatenate()
    x = concat([x, skip])


  # This is the last layer of the model
  last = tf.keras.layers.Conv2DTranspose(
      output_channels, 3, strides=2,
      padding='same')  #64x64 -> 128x128

  x = last(x)

  return tf.keras.Model(inputs=inputs, outputs=x)

model = unet_model(OUTPUT_CHANNELS)
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])


tf.keras.utils.plot_model(model, show_shapes=True)

def create_mask(pred_mask):
  pred_mask = tf.argmax(pred_mask, axis=-1)
  pred_mask = pred_mask[..., tf.newaxis]
  return pred_mask[0]

def show_predictions(dataset=None, num=1):
  if dataset:
    for image, mask in dataset.take(num):
      pred_mask = model.predict(image)
      display_images([image[0], mask[0], create_mask(pred_mask)])
  else:
    display_images([sample_image, sample_mask,
             create_mask(model.predict(sample_image[tf.newaxis, ...]))])

show_predictions()

from IPython.display import clear_output

class DisplayCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs=None):
    # clear_output(wait=True)
    show_predictions()
    print ('\nSample Prediction after epoch {}\n'.format(epoch+1))

"""## 네트워크의 트레이닝"""

EPOCHS = 20
VAL_SUBSPLITS = 5
TEST_LENGTH = len(test_image_paths)
VALIDATION_STEPS = TEST_LENGTH//BATCH_SIZE//VAL_SUBSPLITS

model_history = model.fit(train_dataset, epochs=EPOCHS,
                          validation_steps=VALIDATION_STEPS,
                          validation_data=test_dataset)

loss = model_history.history['loss']
val_loss = model_history.history['val_loss']
epochs = range(EPOCHS)

plt.figure()
plt.plot(epochs, loss, 'r', label='Training loss')
plt.plot(epochs, val_loss, 'bo', label='Validation loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss Value')
plt.ylim([0, 1])
plt.legend()
plt.show()

show_predictions(test_dataset, 10)
#여기까지가 모델을 만들고 학습하는 단계(수업시간까지의 범위)
#-----------------------------------------------

#-----------------------------------------------------------------
#여기서 부터 문제.
test_loss, test_acc = model.evaluate(test_dataset, verbose=2)
print(test_acc)

print("테스트에 대한 정확도는 {}% 입니다.".format(test_acc*100))