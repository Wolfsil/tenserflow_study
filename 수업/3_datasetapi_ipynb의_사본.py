# -*- coding: utf-8 -*-
"""3.DatasetAPI.ipynb의 사본

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Nd5aBvQMLtVaLrmJ3YTH62Z_T6YO-erl

# tf.data: TensorFlow 입력 파이프라인

이 강좌는 `tf.data` API에 대한 공식 [튜토리얼](https://www.tensorflow.org/guide/data)을 우리 수업에 맞게 수정한 것이다.

지금까지의 강좌에서(`MyFirstNN`, `Regression`, 그리고 `CNN`) 학습에 사용될 데이터는 통채로 numpy 배열로 읽혀져와서 신경망을 트레이닝하는 fit 함수에 제공되었다. 아래의 세 경우 모두 `train_features`와 `train_labels`는 numpy array였다.

```
# My First NN의 경우
model.fit(train_features, train_labels, epochs=100,   
    validation_data=(test_features,test_labels),
    callbacks=[cp_callback])

# Regression의 경우
history = dnn_model.fit(
    train_features, train_labels,
    validation_split=0.2,
    verbose=0, epochs=100)

# CNN의 경우
model.fit(train_features, train_labels, epochs=50, 
    validation_data = (val_features, val_labels),
    callbacks=[cp_callback])
```

학습 데이터의 양이 많지 않을 경우에는 이런 방식이 문제가 없지만 대량의 학습 데이터를 사용하는 경우에는 전체 데이터를 메모리에 로드한다는 점에서 불가능하거나 혹은 매우 비효율적이다.

일반적으로 학습 데이터는 신경망에 투입되기 전에 다음과 같은 몇몇 과정을 거쳐야 한다.

* 파일로부터 읽기
* 전처리: 변환(이미지의 크기 변경, Data augmentation 등), 정규화, 인코딩(one-hot) 등
* 랜덤 셔플링
* 배치(batch) 만들기

이중 개별 데이터에 대해서 이루어지는 전처리는 모든 데이터를 로드하지 않고 각각에 대해서 미리 해 둘 수 있지만 셔플링이나 배치로 나누기 등은 그렇지 않다.

텐서플로의 `tf.data` API는 대량의 데이터에 대해서 이런 복잡한 입력 pipeline을 간단하고 효율적이며 일관된 방식으로 구성하게 해준다는 점에서 매우 유용하다.

`tf.data` API는 데이터셋을 표현하는 `tf.data.Dataset` 클래스를 제공한다. 

데이터셋은 두 가지 방식으로 구성할 수 있다.

* 메모리나 파일에 저장된 데이터 소스로부터 `Dataset`객체를 생성한다.
* 하나 혹은 그 이상의 `Dataset`객체을 변환(transformation)하여 새로운 데이터셋을 생성한다.
"""

import tensorflow as tf

import pathlib
import os
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

np.set_printoptions(precision=4)   # 실수를 출력할 때 4자리까지만...

"""## 기본 메카니즘

`Dataset을` 생성하는 가장 간단한 경우는 모든 데이터가 이미 메모리에 로드되어 있는 경우이다. 이 경우 `tf.data.Dataset.from_tensors()` 혹은 `tf.data.Dataset.from_tensor_slices()` 함수로 `Dataset` 객체를 생성할 수 있다. 

만약 데이터가 Tensorflow가 권장하는 데이터 포맷인 TFRecord 파일로 저장되어 있다면 `tf.data.TFRecordDataset()`을 이용하여 `Dataset` 객체를 생성하며, 그 외에도 여러 유형의 데이터 소스로부터 `Dataset` 객체를 생성하는 방법이 제공된다.

일단 `Dataset` 객체가 생성되면 그것에 일련의 메서드 호출을 적용하여 새로운 `Dataset` 객체로 변환(transform)할 수 있다. 예를 들어 `Dataset.map()`을 이용하여  데이터 각각에 대한 어떤 변환을 수행할 수도 있고, `Dataset.shuffle()`은 원소들을 랜덤하게 shuffling하고, `Dataset.batch()`을 이용하여 여러 원소들을 배치로 묶을 수도 있고, `Dataset.repeat()`는 데이터셋으로부터 여러 번 반복하여 데이터를 꺼낼 수 있게 한다. 

이렇게 생성된 `Dataset`은 기본적으로는 Python `iterable` 객체이다. 즉, 거기에 속한 원소들을 순차적으로 iterate하기 위한 iterator를 만들어서 사용할 수 있고, 혹은 간단하게 for문을 사용해서 iterate할 수도 있다.
"""

dataset = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1])
print(dataset)
print(dataset.element_spec)

for elem in dataset:
  print(elem)
  print(elem.numpy())

"""혹은 Python 빌트인 함수인 `iter`를 사용하여 Python iterator를 생성하고 `next` 함수로 원소들을 iterate할 수도 있다. (Python iterator는 `hasNext()` 등과 같이 끝을 검사하는 메서드를 제공하지 않으므로 아래와 같이 `Exception`을 이용하여 끝을 감지하였다.)"""

it = iter(dataset)
while True:
  try:
    print(next(it).numpy())
  except Exception as e:
    break

"""<!-- TODO(jsimsa): Talk about `tf.function` support. -->

<a id="dataset_structure"></a>
### 데이터셋의 구조: `element_spec`

하나의 데이터셋은 동일한 구조를 가지는 원소들로 구성되며, 각각의 원소들의 구조는 `tf.TypeSpec`으로 표현 가능한 타입인데, 예를 들면 `tf.Tensor`, `tf.sparse.SparseTensor`, `tf.RaggedTensor`,
`tf.TensorArray`, `tf.data.Dataset`, 혹은 이들이 중첩된(nested) 구조 등이다.

`Dataset.element_spec` 속성은 데이터셋의 각 원소의 타입을 보여준다. 예를 들면:
"""

print(tf.random.uniform([4, 10]))
dataset1 = tf.data.Dataset.from_tensor_slices(tf.random.uniform([4, 10]))
print(dataset1.element_spec)

for elem in dataset1:
  print(elem)
  print(elem.numpy())

dataset2 = tf.data.Dataset.from_tensor_slices(
   (tf.random.uniform([4]),
    tf.random.uniform([4, 100], maxval=100, dtype=tf.int32)))

print(dataset2.element_spec)

for elem in dataset2:
  print(elem)

for elem1, elem2 in dataset2:
  print(elem1.numpy())
  print(elem2.numpy())

dataset3 = tf.data.Dataset.zip((dataset1, dataset2))
print(dataset3.element_spec)
for elem in dataset3:
  print(elem)

"""이미 만들어진 `Dataset` 객체들을 결합하거나 변형하여 새로운 `Dataset`을 만들 수 있다. """

for a, (b, c) in dataset3:
  print('shapes: {x.shape}, {y.shape}, {z.shape}'.format(x=a, y=b, z=c))

"""## 데이터셋의 생성

### NumPy 배열로 부터

모든 데이터를 numpy 배열의 형태로 메모리에 읽어올 수 있다면 `Dataset` 객체를 만드는 가장 간단한 방법은 `Dataset.from_tensor_slices()` 함수를 사용하는 것이다.
"""

train, test = tf.keras.datasets.fashion_mnist.load_data()

# print(train)            # train is a tuple of two numpy arrays
images, labels = train    # images and labels are numpy arrays
images = images/255

print(type(images))  
print(len(labels)) 

dataset = tf.data.Dataset.from_tensor_slices((images, labels))
print(dataset.element_spec)

for img, label in dataset.take(3):
  plt.imshow(img)
  plt.show()
  print(label.numpy())

"""### Python generator로 부터

`tf.data.Dataset`을 만드는 다른 한가지 방법은 python generator를 사용하는 것이다.
다음 예제는 Python generator의 개념과 사용 방법을 보여준다.
"""

def count(stop):
  i = 0
  while i<stop:
    yield i    # pause
    i += 1

it = count(5)    # iterator it
print(it)
print(next(it))
print(next(it))
print(next(it))
print(next(it))

for n in count(5):
  print(n)

"""`Dataset.from_generator` 함수는 python generator를 데이터 소스로 하는  `Dataset` 객체를 생성해준다. 하나의 Python generator를 입력 매개변수로 받으며, 이때 optial 매개변수 `args`는 generator에게 넘겨줄 매개변수이다. 이외에도 추가적으로 `output_types`과 `output_shapes`를 지정해야 한다."""

# ds_counter = tf.data.Dataset.from_generator(count, args=[25], output_types=tf.int32)
ds_counter = tf.data.Dataset.from_generator(count, args=[25], output_types=tf.int32, output_shapes = ())

print(ds_counter.element_spec)

for ele in ds_counter.take(3):
  print(ele.numpy())

"""`output_shapes` 매개변수는 필수는 아니지만 권장된다. 왜냐하면 많은 tensorflow operation들이 `rank`가 지정되지 않은 텐서를 지원하지 않기 때문이다. 특정 축의 길이가 가변적인 경우에는 `output_shapes`을 지정할 때 그 부분을 `None`으로 명시하라. (즉, 축의 길이는 가변일 수 있지만 축의 개수는 지정되어야 한다.)

아래의 generator는 하나의 정수와 하나의 배열의 tuple을 반환하는데 배열의 길이는 가변적이다. 
"""

def gen_series():
  i = 0
  while True:
    size = np.random.randint(0, 10)
    yield i, np.random.normal(size=(size,))
    i += 1

for i, series in gen_series():
  print(i, ":", str(series))
  if i > 5:
    break

ds_series = tf.data.Dataset.from_generator(
    gen_series, 
    output_types=(tf.int32, tf.float32), 
    output_shapes=((), (None,)))

print(ds_series.element_spec)

"""신경망은 일반적으로 동일한 길이의 입력을 요구한다. 이렇게 가변적인 길이의 데이터를 공급하는 Dataset은 `Dataset.padded_batch` 변환을 이용하여 dummy 값을 추가하여 데이터의 크기를 일정하게 만들수 있다."""

ds_series_batch = ds_series.shuffle(20).padded_batch(batch_size=10)

ids, sequence_batch = next(iter(ds_series_batch))
print(ids.numpy())
print()
print(sequence_batch.numpy())
print()

ds_series_batch_fixed_size = ds_series.shuffle(20).padded_batch(batch_size=5, padded_shapes=((), (10,)), padding_values=(0, -1.0))
ids, sequence_batch = next(iter(ds_series_batch_fixed_size))
print(ids.numpy())
print()
print(sequence_batch.numpy())

"""Python generator를 이용하여 데이터셋을 구성하는 좀 더 실제적인 예를 위해서 이미지 파일들로 구성된 데이터셋을 python generator를 이용해 만들어보자.

먼저 데이터를 다운로드 한다.
"""

flowers = tf.keras.utils.get_file(
    'flower_photos',
    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',
    untar=True)

print(type(flowers), flowers)
flowers_root = os.path.join(os.path.dirname(flowers), 'flower_photos')
print(flowers_root)
class_names = os.listdir(flowers_root)
print(class_names)

"""불필요한 파일 `LICENSE.txt`를 삭제한다."""

os.remove(os.path.join(flowers_root, 'LICENSE.txt'))
class_names = os.listdir(flowers_root) 
print(class_names)

"""Keras는 위에서 `tf.keras.preprocessing.image.ImageDataGenerator`를 제공한다. 이것을 이용하면 generator로 부터 간편하게 Dataset을 만들 수 있으며, **또한 data augmentation을 위한 다양한 이미지의 변환 기능을 제공한다**."""

img_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, rotation_range=20)

print(img_gen)

it = img_gen.flow_from_directory(flowers_root)
print(it)
images, labels = next(it)

print(type(images), images.dtype, images.shape)
print(type(labels), labels.dtype, labels.shape)    # one-hot encoded

def gen_it():
  return img_gen.flow_from_directory(flowers_root)

ds = tf.data.Dataset.from_generator(
    gen_it,
    # lambda: img_gen.flow_from_directory(flowers_root), 
    output_types=(tf.float32, tf.float32), 
    output_shapes=([32,256,256,3], [32,5])
)

print(ds.element_spec)

for images, label in ds.take(1):
  print('images.shape: ', images.shape)
  print('labels.shape: ', labels.shape)

"""**참조:** 동물 이미지를 5개로 분류하는 우리의 예제를 `ImageDataGenerator`를 이용하여 [작성](https://www.dropbox.com/s/7xmwjgdzpdmbuoh/MyFirstNN_DataAPI_Using_Generator.py?dl=0)해보았다.

### TFRecord 파일로부터

`tf.data` API는 메모리에 한 번에 로드할 수 없을 정도로 큰 데이터셋을 위해서 다양한 파일 포맷을 지원하는데 그 중 대표적인 것이 `TFRecord` 파일 포맷이다. TFRecord 파일을 사용하는 자세한 방법은 [튜토리얼](https://www.tensorflow.org/tutorials/load_data/tfrecord)을 참조하라.

예제 테스트 파일을 French Street Name Signs (FSNS)로 부터 다운로드한다.
"""

# Creates a dataset that reads all of the examples from two files.
fsns_test_file = tf.keras.utils.get_file("fsns.tfrec", "https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001")
print(fsns_test_file)

"""`TFRecordDataset`를 생성할 때 하나 혹은 그 이상의 `TFRecord` 파일을 `filenames` 매개변수를 통해 제공할 수 있다.

"""

dataset = tf.data.TFRecordDataset(filenames = [fsns_test_file])
print(dataset.element_spec)

"""일반적으로 `TFRecord` 파일에는 직렬화된 `tf.train.Example` 레코드들이 사용된다. 이 레코드들은 먼저 적절하게 디코딩되어야한다.  """

raw_example = next(iter(dataset))
parsed = tf.train.Example.FromString(raw_example.numpy())

parsed.features.feature['image/text']

"""
### 텍스트 데이터로 부터

텍스트 데이터로부터 `Dataset`을 구성하여 사용하는 완전한 예제를 위해서는 [Loading Text](../tutorials/load_data/text.ipynb)를 참조하라.

많은 데이터 셋이 텍스트 파일들의 형태를 가진다. 
`tf.data.TextLineDataset`은 하나 혹은 그 이상의 텍스트 파일들로 부터 라인(line)들을 뽑아내는 간단한 방법의 하나이다. `TextLineDataset`은 파일들의 각 라인을 하나의 string으로 뽑아낸다. """

directory_url = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'
file_names = ['cowper.txt', 'derby.txt', 'butler.txt']

file_paths = [
    tf.keras.utils.get_file(file_name, directory_url + file_name) for file_name in file_names
]
print(file_paths)

dataset = tf.data.TextLineDataset(file_paths)
print(dataset.element_spec)

"""다음은 첫 번째 파일의 처음 몇 라인들이다."""

for line in dataset.take(5):
  print(line.numpy())
  print(line.numpy().decode("utf-8"))

"""파일을 번갈아가면서 라인들을 뽑아내려면 `Dataset.interleave`를 사용하라. 예를 들어 다음은 각 파일의 첫째, 둘째, 셋째 라인들이다:"""

files_ds = tf.data.Dataset.from_tensor_slices(file_paths)
lines_ds = files_ds.interleave(tf.data.TextLineDataset, cycle_length=3)

for i, line in enumerate(lines_ds.take(9)):
  if i % 3 == 0:
    print()
  print(line.numpy())

"""`TextLineDataset`은 기본적으로는 각 파일의 모든 라인들을 추출한다. 하지만 때로는 파일의 해더 라인이나 코멘트라인 등과 같이 생략하고 싶은 라인들이 있을 수 있다. 이 경우 `Dataset.skip()`이나 
`Dataset.filter()` 등을 이용하여 불필요한 라인들을 건너뛰거나 생략할 수 있다.
"""

titanic_file = tf.keras.utils.get_file("train.csv", "https://storage.googleapis.com/tf-datasets/titanic/train.csv")
titanic_lines = tf.data.TextLineDataset(titanic_file)

for line in titanic_lines.take(10):
  print(line.numpy())

def survived(line):
  return tf.not_equal(tf.strings.substr(line, 0, 1), "0")

survivors = titanic_lines.skip(1).filter(survived)

for line in survivors.take(10):
  print(line.numpy())

"""### CSV 파일로부터

좀 더 자세한 예제는 [Loading CSV Files](../tutorials/load_data/csv.ipynb)와 [Loading Pandas DataFrames](../tutorials/load_data/pandas.ipynb)을 참조하라. 

CSV 파일은 테이블 형태의 데이터를 저장하는 대표적인 파일 포맷이다.
"""

titanic_file = tf.keras.utils.get_file("train.csv", "https://storage.googleapis.com/tf-datasets/titanic/train.csv")

df = pd.read_csv(titanic_file, index_col=None)
df.head()

"""만약 모든 데이터들이 메모리에 로드될 수 있을 정도의 규모라면  `Dataset.from_tensor_slices` 메서드를 이용하여 다음과 같이 데이터셋 객체를 생성한다."""

titanic_slices = tf.data.Dataset.from_tensor_slices(dict(df))

print(titanic_slices.element_spec)

for feature_batch in titanic_slices.take(1):
  for key, value in feature_batch.items():
    print("  {!r:20s}: {}".format(key, value))

"""좀 더 규모가 큰 데이터인 경우라면 저장장치에서 필요할 때만 로드할 수 있다.  `tf.data` 모듈은 하나 혹은 그 이상의 CSV 파일로부터 레코드들을 빼내오는 메서드들을 제공한다.
 `experimental.make_csv_dataset` 함수는 csv 파일들을 읽어오는 고수준 인터페이스를 제공한다.
 이 함수는 column type inference를 지원하고  batching이나  shuffling과 같은 다양한 기능을 제공한다. 
"""

titanic_batches = tf.data.experimental.make_csv_dataset(
    titanic_file, batch_size=4,
    label_name="survived")

for feature_batch, label_batch in titanic_batches.take(1):
  print("'survived': {}".format(label_batch))
  print("features:")
  for key, value in feature_batch.items():
    print("  {!r:20s}: {}".format(key, value))

"""만약 모든 열(column)이 필요하지 않으면 `select_columns` 매개변수를 사용하여 필요한 열만을 선택할 수도 있다."""

titanic_batches = tf.data.experimental.make_csv_dataset(
    titanic_file, batch_size=4,
    label_name="survived", select_columns=['class', 'fare', 'survived'])

for feature_batch, label_batch in titanic_batches.take(1):
  print("'survived': {}".format(label_batch))
  for key, value in feature_batch.items():
    print("  {!r:20s}: {}".format(key, value))

"""### 파일들의 집합으로부터

여러 개의 파일들로 구성되며 각각의 파일이 하나의 데이터에 해당하는 데이테셋들이 있다. 예를 들면 이미지 분류 문제의 경우이다. 위에서 Python generator를 사용하여 이런 경우를 다루었는데 `tf.data.Dataset.list_files`를 이용하여 더 간단하게 `Dataset` 객체를 생성할 수 있다.

우선 예제로 사용할 파일들을 다운로드하자. 여러 종류의 꽃 이미지들로 구성된 데이터셋이다.
"""

flowers_root = tf.keras.utils.get_file(
    'flower_photos',
    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',
    untar=True)
flowers_root = pathlib.Path(flowers_root)
print(type(flowers_root))
print(flowers_root)

"""루트 디렉토리에는 꽃 종류별로 분류된 하위 디렉토리가 있고, 각 하위 디렉토리에는 꽃 이미지 파일들이 있다."""

for item in flowers_root.glob("*"):  
  print(type(item))
  print(item)
  print(item.name)

"""다음과 같이 파일경로들로부터 `Dataset` 객체를 생성한다."""

list_ds = tf.data.Dataset.list_files(str(flowers_root/'*/*'))

for f in list_ds.take(5):
  print(type(f))
  print(f)
  print(f.numpy())
  print(f.numpy().decode("utf-8"))

"""또한 `map` 함수를 이용하여 `process_path` 함수를 데이터셋의 모든 원소들에게 각각 적용한다. `process_path` 함수에서는 `tf.io.read_file` 함수를 이용하여 이미지 파일을 읽고 경로명으로부터 라벨(꽃 종류)을 알아낼 수 있다. """

def process_path(file_path):
  label = tf.strings.split(file_path, os.sep)[-2]
  return tf.io.read_file(file_path), label

labeled_ds = list_ds.map(process_path)

for image_raw, label_text in labeled_ds.take(1):
  img = tf.image.decode_jpeg(image_raw)
  print(img.shape)
  plt.imshow(img)
  plt.show()

  print(image_raw.numpy()[:100])
  print()
  print(label_text.numpy())

"""## Batching

지금까지 데이터셋 객체를 어떻게 생성하는지, 그리고 생성된 데이터셋으로부터 데이터를 iterate하는 것과 `map` 함수를 이용하여 각각의 데이터에 대해서 필요한 전처리를 할 수 있음을 보았다. 

실제로 데이터셋을 학습에 사용하기 위해서는 random batching을 해야한다.

### Simple batching

가장 단순한 형태의 batching은 데이터셋에서 n개의 연속적인 원소를 하나로 묶어 batch로 만드는 것이다. `Dataset.batch()`가 이 일을 해준다. 이 경우 모든 원소들은 정확히 동일한 shape이어야 한다.
"""

inc_dataset = tf.data.Dataset.range(100)
dec_dataset = tf.data.Dataset.range(0, -100, -1)
dataset = tf.data.Dataset.zip((inc_dataset, dec_dataset))

for ele in dataset.take(4):
  print(ele)

batched_dataset = dataset.batch(3)
print(batched_dataset.element_spec)
for batch in batched_dataset:
  print(batch)
  # print([arr.numpy() for arr in batch])

"""데이터셋을 구성하는 데이터의 개수가 배치 크기의 배수가 아닐수도 있으므로 모든 배치가 동일한 shape를 가지지는 않는다. 즉, 마지막 배치의 경우 지정된 배치 크기보다 적은 개수의 원소로 구성될 수 있다. 
그런 이유로 `batched_dataset`의 shape의 어떤 차원의 길이는 `None`이 된다.
"""

print(batched_dataset.element_spec)

"""만약 마지막 배치를 생략하여 모든 배치들이 동일한 크기를 갖도록 하려면 `drop_remainder` 매개변수를 `True`로 설정한다. """

batched_dataset = dataset.batch(7, drop_remainder=True)
print(batched_dataset.element_spec)

"""
### Batching with padding

위의 예는 데이터셋의 원소들이 모두 동일한 크기를 가질 경우이다. 하지만 많은 경우 데이터들이 서로 다른 크기를 가질 수 있다. 그럴 경우 `Dataset.padded_batch` 변환을 이용하여 특정 차원으로 더미(dummy)값을  padding을 함으로써 모든 데이터들이 동일한 크기를 가지도록 만들 수 있다."""

dataset = tf.data.Dataset.range(100)

dataset = dataset.map(lambda x: tf.fill([tf.cast(x, tf.int32)], x))

for batch in dataset.take(4):
  print(batch.numpy())

print()

dataset_padded = dataset.padded_batch(batch_size=4)
for batch in dataset_padded.take(2):
  print(batch.numpy())
  print()

dataset_padded_fixed = dataset.padded_batch(4, padded_shapes=(10,))
for batch in dataset_padded_fixed.take(2):
  print(batch.numpy())
  print()

"""## 데이터셋의 반복 사용: `repeat()`

데이터셋은 트레이닝 과정에서 한 번만 사용되는 것이 아니라 여러 epoch를 거치면서 반복 사용된다. 두 가지 방법이 있다.

가장 간단한 방법은 `Dataset.repeat()` 변환을 이용하는 것이다.
"""

titanic_file = tf.keras.utils.get_file("train.csv", "https://storage.googleapis.com/tf-datasets/titanic/train.csv")
titanic_lines = tf.data.TextLineDataset(titanic_file)

def plot_batch_sizes(ds):
  batch_sizes = [batch.shape[0] for batch in ds]
  plt.bar(range(len(batch_sizes)), batch_sizes)
  plt.xlabel('Batch number')
  plt.ylabel('Batch size')

"""`Dataset.repeat()` 변환을 매개변수가 없이 적용하면 데이터셋으로부터 무한히 반복하여 입력을 받을 수 있다. 

`Dataset.repeat` 변환은 데이터의 끝을 처음과 연결하기 때문에 만약 `Dataset.repeat`를 먼저 적용하고  `Dataset.batch`를 나중에 적용하면 epoch의 끝이 정확하게 구분되지 않는다. 또한 이 결과로 가장 마지막 batch를 제외한 다른 batch 들은 모두 동일한 크기를 가지게 된다.


"""

titanic_batches = titanic_lines.repeat(3).batch(128)
plot_batch_sizes(titanic_batches)

"""만약 epoch를 정확하게 구분하려면 `Dataset.batch`를 먼저 적용하면 된다."""

titanic_batches = titanic_lines.batch(128).repeat(3)

plot_batch_sizes(titanic_batches)

"""만약 각 epoch가 종료될때 마다 어떤 추가적인 작업(예를 들어 통계를 수집한다든가 등)을 하고 싶다면 
매 epoch 마다 dataset iteration을 새로 시작하는 것이 가장 간단한 방법이다.

"""

epochs = 3
dataset = titanic_lines.batch(128)

for epoch in range(epochs):
  for batch in dataset:
    print(batch.shape)
  print("End of epoch: ", epoch)

"""## 랜덤 shuffling

`Dataset.shuffle()` 변환은 고정된 크기의 버퍼를 유지하면서 다음 데이터 원소를 그 버퍼에서 랜덤하게 선택하는 방식으로 데이터를 shuffle한다. 따라서 버퍼의 크기가 클수록 진정한 랜덤 셔플링에 가까워지지만 대신 더 많은 메모리와 시간을 사용하게 된다.

shuffling의 효과를 확인하기 위해서 데이터들에게 index를 부여해보자.
"""

lines = tf.data.TextLineDataset(titanic_file)
counter = tf.data.experimental.Counter()

dataset = tf.data.Dataset.zip((counter, lines))
dataset = dataset.shuffle(buffer_size=100)
dataset = dataset.batch(20)
print(dataset)

"""`buffer_size`가 100이고 batch size는 20이므로 첫 번째 배치에는 index가 120 이상인 원소가 포함되지 않는다. (버퍼에서 순차적으로 하나를 택해서 뽑아낼때마다 그 자리를 다른 원소가 채우기 때문에 120이다.)"""

n,line_batch = next(iter(dataset))
print(n.numpy())

"""`Dataset.batch`와 마찬가지로 shuffle의 경우에도  `Dataset.repeat()` 보다 먼저 적용하느냐 나중에 적용하느냐에 달라지는 점이 있다.

`Dataset.shuffle`은 버퍼가 empty가 되기 전에는 epoch의 끝을 알수 없으므로 repeat보다 shuffle을 먼저 적용하면 각 epoch의 끝이 구분되게 된다.  
"""

dataset = tf.data.Dataset.zip((counter, lines))
shuffled = dataset.shuffle(buffer_size=100).batch(10).repeat(2)

print("Here are the item ID's near the epoch boundary:\n")
for n, line_batch in shuffled.skip(60).take(5):
  print(n.numpy())

shuffle_repeat = [n.numpy().mean() for n, line_batch in shuffled]
plt.plot(shuffle_repeat, label="shuffle().repeat()")
plt.ylabel("Mean item ID")
plt.legend()

"""하지만 shuffle보다  repeat를 먼저 적용하면 epoch boundary가 구분되지 않는다."""

dataset = tf.data.Dataset.zip((counter, lines))
shuffled = dataset.repeat(2).shuffle(buffer_size=100).batch(10)

print("Here are the item ID's near the epoch boundary:\n")
for n, line_batch in shuffled.skip(55).take(15):
  print(n.numpy())

repeat_shuffle = [n.numpy().mean() for n, line_batch in shuffled]

plt.plot(shuffle_repeat, label="shuffle().repeat()")
plt.plot(repeat_shuffle, label="repeat().shuffle()")
plt.ylabel("Mean item ID")
plt.legend()

"""## 개별 데이터의 전처리

`Dataset.map(f)` 변환은 데이터셋의 각 원소에게 주어진 함수 `f`를 적용함으로써 사실상 새로운 데이터셋을 생성하는 셈이 된다. **함수 `f`는 데이터셋에 속한 단일 원소에 해당하는 `tf.Tensor` 객체들을 매개변수로 입력받고 `tf.Tensor` 객체들을 반환한다.** 

여기서는 `Dataset.map()`을 적용하는 대표적인 몇몇 예들을 살펴본다.

### 이미지 데이터의 decode와 resize

신경망에서 이미지를 다룰 때 종종 서로 다른 크기의 이미지들을 동일한 크기로 resize할 필요가 있다.
"""

list_ds = tf.data.Dataset.list_files(str(flowers_root/'*/*'))

"""데이터셋을 구성하는 각각의 원소(경로명)을 처리하는 함수를 작성해보자."""

# Reads an image from a file, decodes it into a dense tensor, and resizes it
# to a fixed shape.
def parse_image(filename):
  parts = tf.strings.split(filename, os.sep)
  label = parts[-2]

  image = tf.io.read_file(filename)
  image = tf.image.decode_jpeg(image)
  image = tf.image.convert_image_dtype(image, tf.float32)
  image = tf.image.resize(image, [128, 128])
  return image, label

"""제대로 동작하는지 테스트해보자."""

file_path = next(iter(list_ds))
image, label = parse_image(file_path)

def show(image, label):
  plt.figure()
  plt.imshow(image)
  plt.title(label.numpy().decode('utf-8'))
  plt.axis('off')

show(image, label)

"""데이터셋에 적용해보자."""

images_ds = list_ds.map(parse_image)

for image, label in images_ds.take(2):
  show(image, label)

"""### Applying arbitrary Python logic

이미지를 resize하는 것과 같은 간단한 전처리는 Tensorflow가 제공하는 연산(`tf.image.resize` 등)을 적용하면 된다. 하지만 종종 좀 더 복잡한 전처리가 필요할 수도 있다. 이런 경우 보통의  Python함수를 적용해야 한다. 이때 `Dataset.map()`에 `tf.py_function()` 연산을 사용할 수 있다.

예를 들어 이미지에 대한 랜덤 회전이 필요하다고 가정해보자. 이런 일은 종종 image augmentation을 위해서 필요하다. `tf.image` 모듈은 오직 90도로 회전하는 `tf.image.rot90` 만을 제공한다. 

**참고:** `tensorflow_addons`은 Tensorflow와 호환되는 `tensorflow_addons.image.rotate`을 제공한다. 

`tf.py_function`의 적용 예를 보기 위해서 `scipy.ndimage.rotate` 함수를 사용해보자. (이 함수는 Tensorflow의 일부가 아니다.)
"""

import scipy.ndimage as ndimage

def random_rotate_image(image):
  image = ndimage.rotate(image, np.random.uniform(-30, 30), reshape=False)
  return image

image, label = next(iter(images_ds))
image = random_rotate_image(image)
show(image, label)

"""이 함수 `random_rotate_image`를 `Dataset.map`에 사용하려면 함수를 적용할 때 return shapes과 types을 명시해야 한다."""

def tf_random_rotate_image(img, label):
  im_shape = img.shape
  [image,] = tf.py_function(random_rotate_image, [img], [tf.float32])
  image.set_shape(im_shape)
  return image, label

rot_ds = images_ds.map(tf_random_rotate_image)

for image, label in rot_ds.take(2):
  show(image, label)

"""## `tf.keras`에서 `tf.data` 사용하기

`tf.keras` API는 데이터셋과 관련된 많은 부분들을 단순화해준다.  `.fit()`과 `.evaluate()` 그리고 `.predict()` API는 데이터셋 자체를 입력으로 받아들인다.
"""

train, test = tf.keras.datasets.fashion_mnist.load_data()

images, labels = train
images = images/255.0
labels = labels.astype(np.int32)

fmnist_train_ds = tf.data.Dataset.from_tensor_slices((images, labels))
fmnist_train_ds = fmnist_train_ds.shuffle(5000).batch(32)

model = tf.keras.Sequential([
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(10)
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), 
              metrics=['accuracy'])

"""단지 `(feature, label)` pairs으로 구성된 데이터셋 자체를 `Model.fit`과 `Model.evaluate` 함수에게 입력으로 제공하는 것으로 충분하다."""

model.fit(fmnist_train_ds, epochs=2)

"""만약 무한히 반복되는 데이터셋을 넘겨주고 싶으면 (예를 들어 `Dataset.repeat()`)  `steps_per_epoch` argument를 지정해야 한다."""

model.fit(fmnist_train_ds.repeat(), epochs=2, steps_per_epoch=20)

"""evaluation에서는 evaluation steps의 횟수를 지정할 수 있다. """

loss, accuracy = model.evaluate(fmnist_train_ds)
print("Loss :", loss)
print("Accuracy :", accuracy)

"""긴 데이터 셋에 대해서는 number of steps를 지정하라:"""

loss, accuracy = model.evaluate(fmnist_train_ds.repeat(), steps=10)
print("Loss :", loss)
print("Accuracy :", accuracy)

"""`Model.predict`를 호출할 때는 label은 불필요하다."""

predict_ds = tf.data.Dataset.from_tensor_slices(images).batch(32)
result = model.predict(predict_ds, steps = 10)
print(result.shape)

"""하지만 label을 포함하는 데이터셋을 넘겨주더라도 label들은 그냥 무시되므로 별 문제가 없다."""

result = model.predict(fmnist_train_ds, steps = 10)
print(result.shape)