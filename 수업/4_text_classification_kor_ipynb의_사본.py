# -*- coding: utf-8 -*-
"""4.Text_classification_Kor.ipynb의 사본

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a2u4TdL1_zwQmhhlQaj0SxE0DqooNI7K

# 영화 리뷰 텍스트의 분류

Note: 이 튜토리얼은 [텐서플로 공식 홈페이지에 있는 것](https://www.tensorflow.org/tutorials/keras/text_classification)을 이 수업의 목적에 맞게 다시 수정한 것이다.

이 강좌에서는 영화 리뷰(review) 텍스트를 **긍정적(positive) 평가**인지 혹은 **부정적(negative) 평가**인지 분류하는 문제를 다룬다. 임의의 텍스트를 2개의 클래스로 분류한다는 의미에서 **이진 분류(binary classification)** 문제이다.

[IMDB](https://www.imdb.com/)(Internet Movie Database)에서 수집한 50,000개의 영화 리뷰 텍스트를 담은 [IMDB 데이터셋](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb)을 사용한다. 25,000개 리뷰는 훈련용으로, 25,000개는 테스트용으로 나뉘어져 있다. 훈련 세트와 테스트 세트 각각에서 긍정적인 리뷰와 부정적인 리뷰의 개수는 동일하다.
"""

import matplotlib.pyplot as plt
import os
import re
import shutil
import string
import tensorflow as tf

from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow.keras import preprocessing
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization

print(tf.__version__)

"""## IMDB 데이터셋 다운로드

다음 코드는 IMDB 데이터셋을 컴퓨터에 다운로드한다:
"""

url = "https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"

dataset = tf.keras.utils.get_file("aclImdb_v1.tar.gz", url,
                                    untar=True, cache_dir='.',
                                    cache_subdir='')

dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')
print(dataset_dir)

os.listdir(dataset_dir)

train_dir = os.path.join(dataset_dir, 'train')
os.listdir(train_dir)

"""디렉토리 `aclImdb/train/pos`과 `aclImdb/train/neg`에는 많은 텍스트 파일들이 있으며 각각은 어떤 영화에 대한 리뷰이다. 그 중 하나를 다음과 같이 살펴보자."""

sample_file = os.path.join(train_dir, 'pos/1181_9.txt')
with open(sample_file) as f:
  print(f.read())

"""## 데이터 로드하기

데이터를 읽어서 학습에 적합한 형태로 준비하기 위해서 `text_dataset_from_directory` 유틸리티를 이용한다. 이 유틸리티를 사용하기 위해서는 디렉토리가 다음과 같은 구조를 가져야 한다. 

```
main_directory/ 
...class_a/ 
......a_text_1.txt 
......a_text_2.txt  
...class_b/ 
......b_text_1.txt 
......b_text_2.txt
```

이진 분류 문제의 경우 두 개의 디렉토리(`class_a`와 `class_b`)가 있어야 한다. 지금의 예에서는 `aclImdb/train/pos`와  `aclImdb/train/neg` 디렉토리에 해당한다. 다운로드된 IMDB 데이터 셋에는 불필요한 폴더 `unsup`가 포함되어 있으므로 다음과 같이 삭제해주어야 한다.
"""



remove_dir = os.path.join(train_dir, 'unsup')
shutil.rmtree(remove_dir)

"""이제 [`text_dataset_from_directory`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text_dataset_from_directory) 유틸리티를 이용하여 `tf.data.Dataset` 인스턴스를 생성한다. `tf.data`는 텐서플로에서 데이터를 다루는 매우 강력한 도구이다. 

기계학습에서는 데이터를 train, test, 그리고 validation 셋으로 나누는 것이 일반적이다.  하지만 IMDB 데이터 셋은 train과 test셋으로만 나누어져 있다. 다음과 같이 매개변수 `validation_split`을 이용하여 train 데이터의 20%를 분할하여 validation 데이터셋으로 사용한다. 

"""

batch_size = 32
seed = 42

raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(
    'aclImdb/train', 
    batch_size=batch_size, 
    validation_split=0.2, 
    subset='training', 
    seed=seed)

print(raw_train_ds.element_spec)

"""확인을 위해서 `raw_train_ds`으로부터 다음과 같이 3개의 샘플 데이터를 출력해본다."""

for text_batch, label_batch in raw_train_ds.take(1):
  for i in range(3):
    print("Review", text_batch.numpy()[i])
    print("Label", label_batch.numpy()[i])

"""리뷰 텍스트에는 쉼표, 마침표 등의 기호들이나 `<br/>` 등과 같은 HTML 태그들이 포함되어 있기도 하다. 이런 것들은 텍스트의 의미와는 무관하므로 모두 제거하는 것이 좋다. 어떻게 제거하는지는 뒤에서 다룬다. 

Label은 정수 0 혹은 1이다. 0과 1 중 누가 positive review에 해당하고 누가 negative review에 해당하는지 알아보려면 다음과 같이 데이터셋의 `class_names` 속성을 체크해보면 된다. 
"""

print("Label 0 corresponds to", raw_train_ds.class_names[0])
print("Label 1 corresponds to", raw_train_ds.class_names[1])

"""다음으로 validation dataset과 test dataset을 생성한다.

**주의:** validation_split과 subset 매개변수를 이용해서 validation dataset을 만들 경우 반드시 random seed를 명시하거나 혹은 shuffle=False로 해주어야 한다. 그래야만 training set과 validation set이 중복되지 않도록 분할된다.
"""

raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(
    'aclImdb/train', 
    batch_size=batch_size, 
    validation_split=0.2, 
    subset='validation', 
    seed=seed)

raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(
    'aclImdb/test', 
    batch_size=batch_size)

"""### 데이터 준비

다음으로 [`preprocessing.TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization) 레이어를 이용하여 데이터를 **표준화(standardize), 토큰으로 나누기(tokenize), 그리고 벡터화(vectorize)**한다.

여기서 표준화(standardization)는 텍스트에 포함된 쉼표, 마침표 등의 기호나 HTML 태그 등을 제거하여 데이터를 단순화하는 것을 의미한다. 
토큰으로 나누는 것은 텍스트를 개별 단어들로 나누는 작업을 의미한다.
벡터화란 나누어진 토큰들을 숫자로 표현하는 것을 말한다. 
이 작업들은 모두 `preprocessing.TextVectorization` 레이어에서 이루어진다.

텍스트의 표준화를 위해서 `tf.strings`가 제공하는 정규표현식(regular expression) 기반의 스트링 매칭 기능을 사용한다. 이 기능을 이해하기 위해 우선 간단한 예제 코드를 살펴보자.
"""

print(string.punctuation)   # 텍스트에 등장할 수 있는 모든 기호들의 집합
print(re.escape(string.punctuation))  # 이 기호들을 모두 escape하며 기호에 부여된 의미를 제거한다.
print('[%s]' % re.escape(string.punctuation))
print(tf.strings.regex_replace('he3jf*&%*%#82../.,}{()))',
                                  '[%s]' % re.escape(string.punctuation),
                                  ''))

"""이제 이 방법을 사용하여 텍스트를 standardize하는 함수를 작성한다. 이 함수는 하나의 리뷰 텍스트를 받아서 HTML 태그와 기호들을 제거한 텍스트를 반환한다."""

def custom_standardization(input_data):
  lowercase = tf.strings.lower(input_data)
  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')
  return tf.strings.regex_replace(stripped_html,
                                  '[%s]' % re.escape(string.punctuation),
                                  '')

"""이제 `TextVectorization` 층을 생성해보자. 각각의 토큰에 대해서 유일한 정수 인덱스를 생성하기 위해서 `output_mode`를 `int`로 설정한다. `output_sequence_length`를 설정하면 문장을 짜르거나 혹은 padding해서 항상 길이가 `sequence_length`가 되도록 해준다. """

max_features = 10000    # 사용될 어휘의 개수의 상한값, 즉 각각의 단어들은 10000 이하의 정수로 표현된다.
sequence_length = 250   # 각 리뷰 텍스트의 최대 길이(단어 수)

vectorize_layer = TextVectorization(
    standardize=custom_standardization,
    max_tokens=max_features,   
    output_mode='int',
    output_sequence_length=sequence_length)

"""다음으로 adapt 함수를 호출하여 전처리 레이어를 데이터셋에 맞춘다. 이렇게 하면 리뷰 텍스트들에 등장하는 어휘들에게 정수 인덱스를 부여할 것이다. 

**주의:** 당연한 말이지만 adapt함수는 오직 training 데이터에 대해서만 적용해야하며 test 데이터를 포함하여 적용해서는 안된다. 
"""

# Make a text-only dataset (without labels), then call adapt
train_text = raw_train_ds.map(lambda x, y: x)  
vectorize_layer.adapt(train_text)

"""이제 이 전처리 층의 결과를 확인해보기 위해서 하나의 함수를 작성해보자."""

def vectorize_text(text, label):
  text = tf.expand_dims(text, -1)
  return vectorize_layer(text), label

# retrieve a batch (of 32 reviews and labels) from the dataset
text_batch, label_batch = next(iter(raw_train_ds))
first_review, first_label = text_batch[0], label_batch[0]
print("Review", first_review)
print("Label", raw_train_ds.class_names[first_label])
print("Vectorized review", vectorize_text(first_review, first_label))

"""결과에서 보이듯이 각각의 토큰은 10000 이하의 하나의 정수로 표현되었다. `.get_vocabulary()` 함수를 호출하여 각각의 단어가 어떤 정수로 표현되었는지 확인할 수 있다."""

print("1287 ---> ",vectorize_layer.get_vocabulary()[1287])
print(" 313 ---> ",vectorize_layer.get_vocabulary()[313])
print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))

"""이제 준비가 거의 끝났다. 전처리의 마지막 단계로 각각의 데이테 셋에 지금까지 만든 `TextVectorization` 층을 적용한다."""

train_ds = raw_train_ds.map(vectorize_text)
val_ds = raw_val_ds.map(vectorize_text)
test_ds = raw_test_ds.map(vectorize_text)

"""## 성능 개선을 위한 데이터셋 설정

데이터셋을 사용할 떄 성능의 개선을 위해서 두 가지 방법을 적용한다. 

`.cache()`는 저장장치에서 로드한 데이터를 메모리에 유지하도록 해준다. 또한 `.prefetch()`는 데이터의 전처리와 모델 실행을 중첩해서 입출력이 병목이 되는 것을 방지한다.
"""

AUTOTUNE = tf.data.experimental.AUTOTUNE

train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)
test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)

"""## Word embeddings

### 단어를 숫자로 표현하기

지금 현재 데이터 셋에서 각각의 단어는 하나의 유일한 정수로 표현되어 있다. 가령 "cat"은 1로, "mat"은 2로, 등이다. 그러면 예를 들어 문장 "The cat sat on the mat"은 벡터 [5, 1, 4, 3, 5, 2]로 표현될 것이다. 이렇게 각각의 단어를 인덱스로 표현하는 방법은 효율적이기는 하다. 하지만 이런 정수 인코딩은 arbitrary하다. 즉 **단어들 간의 어떤 연관성도 표현하지 못한다**. 가령 유사한 단어라고 해서 유사한 정수값으로 표현되지는 않는다. **그러면 어떤 문제가 있을까?**

다른 방법으로 one-hot 인코딩을 생각할 수 있다. 예를 들어 입력 데이터에 등장하는 모든 단어의 목록이 (cat, mat, on, sat, the)라고 하자. 그러면 각각의 단어를 다음과 같이 하나의 길이 5인 벡터로 표현하는 것이다. 

<img src="https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/one-hot.png?raw=1" alt="Diagram of one-hot encodings" width="400" />

하지만 이 방법은 매우 비효율적이다. 만약 사전에 10,000개의 단어가 있다면 각각의 단어가 길이가 10,000인 sparse 벡터(대부분의 값이 0인 벡터)로 표현될 것이다.

### Word embeddings

Word embedding의 목적은 각 단어를 **dense한 벡터**로 표현하면서 또한 **유사한 단어는 유사한 벡터로 표현**되도록 하는 것이다. 하지만 단어의 유사성은 매우 모호한 개념이어서 수작업이나 혹은 어떤 전통적인 의미의 알고리즘을 사용하여 하기는 어렵다. 

대신 embedding 자체를 신경망을 이용하여 학습한다. 이 말은 단어를 고정된 길이의 벡터로 변환하는 신경망을 구성하고 이 신경망을 훈련한다는 의미이다. 

이때 물론 각 단어를 표현할 벡터의 길이는 지정해주어야 한다. 작은 데이터셋에 대해서는 8차원 벡터를 사용하기도 하고 큰 데이터 셋에 대해서는 1024-차원 벡터를 사용하기도 한다. 긴 벡터를 사용할 수록 단어들 간의 정교한 관계를 포착할 수 있겠지만 대신 더 많은 시간이 걸릴 것이다. 

<img src="https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/embedding2.png?raw=1" alt="Diagram of an embedding" width="400"/>

위의 그림은 word embedding의 결과를 보여준다. 각 단어는 4-차원 실수 벡터로 표현되었다.

### Keras의 Embedding Layer

일반적으로 Word embedding을 수행하는 신경망의 구조는 `Word2Vec`, `GloVe` 등 매우 다양하다. 
Keras의 `Embedding` layer가 정확히 어떤 구조의 네트워크를 사용하는지는 Keras의 소스코드를 분석해보지 않고서는 알기 어렵다. 

다만 Keras의 `Embedding` 층은 하나 혹은 그 이상의 층으로 구성된 일종의 신경망이며, 단어를 고정 길이의 실수 벡터로 변환하는 역할을 한다. 이 신경망의 가중치들은 미리 학습되어 있는 것이 아니므로 처음에는 단어들을 거의 랜덤한 벡터로 변환할 것이다. 하지만 `Embedding`층은 그것을 포함하는 전체 신경망의 일부이므로 학습 과정을 거쳐 목적에 부합하는 방식으로 단어들을 벡터로 변환하도록 훈련되어 간다는 것이다. 

단, 자연어에 대해서 범용의 목적으로 미리 학습된(pre-trained) Embedding 네트워크들이 있으며, 어떤 응용에서는 이것을 사용하는 것이 좋을 수 있다.

## 모델 구성
"""

embedding_dim = 16   # 각 단어를 길이 16인 벡터로 embedding한다.

model = tf.keras.Sequential([
  layers.Embedding(max_features + 1, embedding_dim),   # 첫 번째 매개변수는 size of the vocabulary이다. 0은 어떤 단어도 표현하지 않는다. 그래서 max_features+1이다.
  layers.Dropout(0.2),
  layers.GlobalAveragePooling1D(),
  layers.Dropout(0.2),
  layers.Dense(1)])

model.summary()

"""첫 째 층은 Embedding layer이다. 이 층은 정수로 인코딩된 리뷰 텍스트를 입력받아서 각 단어(를 표현하는 정수 인덱스)를 임베딩 벡터로 변환한다. 이 벡터들은 모델이 훈련되는 과정에서 학습된다. 각 정수 인덱스가 하나의 벡터로 변환되므로 출력의 차원은 `(batch, sequence, embedding)`이 된다.

다음은 `GlobalAveragePooling1D` 층이며 CNN에서 사용되는 Pooling 층들처럼 입력 데이터를 축약하는 역할을 한다. 여기에서는 각 문장은 `sequence`개의 단어로 구성되고 각 단어는 길이 `embedding`인 벡터로 표현되는데, 이 문장들을 길이가 `sequence`인 축으로 평균하여 길이 `embedding`인 하나의 벡터로 축약한다. 이것은 서로 다른 길이의 입력을 동일한 길이의 벡터로 변환하는 가장 간단한 방법이다.

이렇게 고정된 길이로 변환된 입력은 16개의 노드를 가진 Dense 층을 거쳐 단일 출력값으로 변환된다. 마지막 층은 `sigmoid`은 활성화 함수를 사용하여 0과 1 사이의 실수를 출력한다. 0에 가까울 수록 negative, 1에 가까울 수록 positive한 리뷰에 해당한다. 즉, 출력값은 positive한 리뷰일 확률로 해석한다.

### 손실 함수와 옵티마이저

이 예제는 이진 분류 문제이고 모델이 확률을 출력하므로, `binary_crossentropy` 손실 함수를 사용한다. 예를 들어 `mean_squared_error`와 같은 다른 손실함수를 사용할 수도 있지만 일반적으로 `binary_crossentropy`가 확률을 다루는데 적합하다.
"""

model.compile(loss=losses.BinaryCrossentropy(from_logits=True), # from_logits=True이면 네트워크의 출력을 확률이라는 의미이고, 이렇게 하면 계산이 좀 더 안정적이 된다.
              optimizer='adam', 
              metrics=tf.metrics.BinaryAccuracy(threshold=0.5))

"""## 모델 훈련"""

epochs = 10
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=epochs)

"""## 모델 평가

모델의 성능을 확인해 보자. 두 개의 값이 반환된다. 손실(오차를 나타내는 숫자이므로 낮을수록 좋다)과 정확도이다.
"""

loss, accuracy = model.evaluate(test_ds)

print("Loss: ", loss)
print("Accuracy: ", accuracy)

"""이 예제는 매우 단순한 모델을 사용하므로 86% 정도의 정확도를 달성했다.

## 정확도와 손실 그래프 그리기

`model.fit()`은 `History` 객체를 반환한다. 여기에는 훈련하는 동안 일어난 모든 정보가 담긴 딕셔너리(dictionary)가 들어 있다:
"""

history_dict = history.history
history_dict.keys()

"""네 개의 항목이 있다. 훈련과 검증 단계에서 모니터링하는 지표들이다. 훈련 손실과 검증 손실을 그래프로 그려 보고, 훈련 정확도와 검증 정확도도 그래프로 그려서 비교해 보자:"""

acc = history_dict['binary_accuracy']
val_acc = history_dict['val_binary_accuracy']
loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(acc) + 1)

# "bo" is for "blue dot"
plt.plot(epochs, loss, 'bo', label='Training loss')
# b is for "solid blue line"
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')

plt.show()

"""이 그래프에서 점선은 훈련 손실과 훈련 정확도를 나타내고 실선은 검증 손실과 검증 정확도이다.
훈련 손실은 에포크마다 *감소*하고 훈련 정확도는 *증가*하였음을 확인할 수 있는데, 이는 일반적으로 당연한 것이다.

하지만 검증 손실과 검증 정확도는 일정 지점 이후에는 더이상 개선되지 않는다. 그 이후의 학습은 일종의 과대적합(overfitting)으로 볼 수 있다. 즉, 이 지점부터는 모델이 과도하게 최적화되어 *일반화*되기 어려운 훈련 데이터의 특정 측면을 학습한 것이다.

**참고**:  `Earlystopping callback`을 사용하여 과대적합이 시작되는 시점에 자동으로 훈련을 멈출수도 있다.

## Export the model

위의 코드에서 `TextVectorization` 층은 Sequential 모델인 `model` 내에 포함되어 있지 않고 바깥에 존재한다. 
이 네트워크를 export해서 배포하려면 `TextVectorization` 층을 export할 모델 내에 포함해야 할 것이다. 이를 위해서 이미 훈련된 모델인 `model`을 이용하여 새로운 모델을 생성한다.
"""

export_model = tf.keras.Sequential([
  vectorize_layer,
  model,
  layers.Activation('sigmoid')
])

export_model.compile(
    loss=losses.BinaryCrossentropy(from_logits=False), optimizer="adam", metrics=['accuracy']
)

# Test it with `raw_test_ds`, which yields raw strings
loss, accuracy = export_model.evaluate(raw_test_ds)
print(accuracy)

"""**참고**: 이렇게 텍스트 전처리를 모델 내부에 넣는 것은 모델을 export하고 deploy하는 것을 쉽게 만든다. 하지만 한 가지 단점이 있다. 전처리 층이 모델의 외부에 있으면 모델이 GPU에서 훈련되는 동안 비동기적인 방식으로 CPU에 의해서 버퍼링과 전처리가 수행될 수 있으므로 성능이 개선될 수 있다. 
따라서 GPU를 사용하는 경우에는 모델을 훈련하는 과정에서는 전처리를 모델 외부에 배치하고 최종적으로 모델을 export할 때 위에서 위에서 보여준 것처럼 모델의 내부에 넣는 것이 효과적이다.

## Inference on new data

이제 새로운 데이터에 대해서 판정하려면 `model.predict()`를 사용하면 된다.
"""

examples = [
  "The movie was great!",
  "The movie was okay.",
  "The movie was terrible..."
]

export_model.predict(examples)

"""## Save the entire model

[`model.save`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#save)를 호출하여 모델의 구조와 가중값을 하나의 파일로 저장하고, 별개의 Python코드에서 다시 불러와 사용할 수 있다.

모델은 두 가장 형식(`SavedModel`과 `HDF5` 형식)으로 저장될 수 있는데 여기서는 `SavedModel`에 대해서만 다루겠다. 
"""

# Save the entire model as a SavedModel.
!mkdir -p saved_model
export_model.save('saved_model/my_model')

# my_model directory
!ls saved_model

# Contains an assets folder, saved_model.pb, and variables folder.
!ls saved_model/my_model

new_model = tf.keras.models.load_model('saved_model/my_model',
                                       custom_objects={'TextVectorization':TextVectorization, 
                                      'custom_standardization':custom_standardization})

# Check its architecture
new_model.summary()

# Evaluate the restored model
loss, accuracy = new_model.evaluate(raw_test_ds)
print(accuracy)

"""## **연습**: multiclass classification on Stack Overflow questions

[Stack Overflow](http://stackoverflow.com/)에 올라온 프로그램과 관련된 질문들에 하나의 태그(tag)를 다는 문제를 풀어라. 여기서 태그는 Python, CSharp, JavaScript, 혹은 Java와 같이 질문에 대응하는 프로그래밍 언어를 의미한다. 

사용할 [데이터셋](http://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz)에는 수천개의 질문들과 각 질문마다 하나의 태그가 달려 있다. 

데이터셋을 다운로드하여 압축을 풀면 아래와 같은 디렉토리 구조를 가진다. 

```
train/
...python/
......0.txt
......1.txt
...javascript/
......0.txt
......1.txt
...csharp/
......0.txt
......1.txt
...java/
......0.txt
......1.txt
```

**참고**: 문제가 너무 쉬워지는 것을 방지하기 위해서 질문들에서 Python, CSharp, JavaScript, 혹은 Java와 같이 직접적으로 답에 해당하는 단어들을 고의적으로 제거하였다.
"""